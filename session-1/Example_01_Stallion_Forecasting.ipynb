{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53950f4b-802b-4ee3-9bd6-5314b6f8f5e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Setup libraries & Import it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b17deaf2-067e-4e02-bf43-5377c0c20285",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install openpyxl polars lightgbm xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83656498-235a-45ee-a3a8-30f86b31d9d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad710e4e-c119-425a-8490-ed826646b6ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Decorator to calculate running time.\n",
    "def with_time_review(func):\n",
    "    import time\n",
    "    import traceback\n",
    "\n",
    "    def wrapper(*args, **kwargs):\n",
    "      begin = time.time()\n",
    "      result = func(*args, **kwargs)\n",
    "      end = round(time.time() - begin, 5)\n",
    "\n",
    "\n",
    "      print (f\"Function: '{func.__name__}' runs for: {end} seconds.\")\n",
    "      print (\"----------------------------\\n\")\n",
    "      return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4851da46-0585-4d32-83ff-859503aad274",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Import parallel libraries\n",
    "import pyspark.pandas as ps_pd\n",
    "import polars as pl\n",
    "import ray\n",
    "import multiprocessing as mp\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Import machine learning model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgbm\n",
    "import xgboost as xgb\n",
    "\n",
    "# Ray Setup\n",
    "ray.init(\n",
    "    log_to_driver=False,\n",
    "    ignore_reinit_error=True,\n",
    "    runtime_env={\n",
    "        \"pip\": [\"lightgbm\", \"xgboost\"], \n",
    "        \"env_vars\": {\"PYTHONHASHSEED\": \"0\", }},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df1fd334-66b9-4626-ac85-fddd82b0c241",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Setup folder and Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c22e2009-e11c-4e13-aff4-51eb201393d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_FOLDER = 'mnt/adls_gen2/HENRYDUCLAI/TRAINING/SESSION_01'\n",
    "os.makedirs('/dbfs/' + OUTPUT_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a4b3789-579c-4731-bdf4-9af94e43b985",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "FILE_PATH = '../dataset/Stallion-AbinBev-kaggle.csv'\n",
    "df_dataset = pd.read_csv(FILE_PATH)\n",
    "df_dataset['KEY'] = df_dataset['Agency'] + '_' + df_dataset['SKU']\n",
    "df_dataset = df_dataset.drop(columns=['Price']) # This PRICE needs to remove due to leakage variables. PRICE = SALES + PROMO\n",
    "spark_df_dataset = spark.createDataFrame(df_dataset)\n",
    "\n",
    "# Quick review dataset\n",
    "display(spark_df_dataset)\n",
    "df_dataset.groupby('KEY').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1a55e2c-f668-4b4e-9558-b9644cd08806",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Forecast module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afdc3049-bfea-4f1d-a8a8-a51ce13c04e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def codeblock_feature_engineering(key, df_group):\n",
    "    df_group = df_group.sort_values(by=['YearMonth'])\n",
    "    df_group = df_group.reset_index(drop=True)\n",
    "    \n",
    "    for col in ['Sales', 'Promotions']:\n",
    "        for lag in range(1, 13):\n",
    "            df_group[f'f__LAG_{col}_{lag}'] = df_group[col].shift(lag)\n",
    "        for window in [3, 6, 9, 12]:\n",
    "            df_group[f'f__MA_{col}_{window}'] = df_group[col].rolling(window).mean().shift(1)\n",
    "            df_group[f'f__MSTD_{col}_{window}'] = df_group[col].rolling(window).std().shift(1)\n",
    "\n",
    "    df_group = df_group.drop(columns=['Agency', 'SKU'])\n",
    "    df_group = df_group.dropna()\n",
    "    return df_group\n",
    "\n",
    "def codeblock_model_forecasting(key, df_train, df_test):\n",
    "    import numpy as np\n",
    "    np.random.seed(1234)\n",
    "\n",
    "    models_list = [\n",
    "        RandomForestRegressor(n_jobs=1, random_state=1234),\n",
    "        lgbm.LGBMRegressor(n_jobs=1, random_state=1234),\n",
    "        xgb.XGBRegressor(n_jobs=1, random_state=1234),\n",
    "    ]\n",
    "\n",
    "    X_train, y_train = df_train.drop(columns=['Sales']), df_train['Sales']\n",
    "    X_test, y_test = df_test.drop(columns=['Sales']), df_test['Sales']\n",
    "\n",
    "    list_numeric_cols = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "    for model in models_list:\n",
    "        model.fit(X_train[list_numeric_cols], y_train)\n",
    "        y_pred = model.predict(X_test[list_numeric_cols])\n",
    "        df_test[model.__class__.__name__] = y_pred\n",
    "    \n",
    "    return df_test  \n",
    "\n",
    "def codeblock_evalutation_pipeline(key, df_group, training_yearmonth):    \n",
    "\n",
    "    df_feature_engineering = codeblock_feature_engineering(key, df_group)\n",
    "\n",
    "    df_train = df_feature_engineering.query(f\"YearMonth < {training_yearmonth}\")\n",
    "    df_test = df_feature_engineering.query(f\"YearMonth >= {training_yearmonth}\")\n",
    "\n",
    "    df_forecast = codeblock_model_forecasting(key, df_train, df_test)\n",
    "    \n",
    "    return df_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "765b0947-632d-4533-b212-2396f59718db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Demo on 1 Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69db5c66-6337-403e-afba-f4635c5514b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "training_yearmonth = 201707\n",
    "key = 'Agency_01_SKU_01'\n",
    "df_group = df_dataset.query(f\"KEY == '{key}'\")\n",
    "df_demo = codeblock_evalutation_pipeline(key, df_group, training_yearmonth)\n",
    "df_schema_spark = spark.createDataFrame(df_demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a06fbe8-7078-4b6c-a21c-77a02da4578c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Parallel setup for Ray and Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b299b0fb-d07c-45cd-ba6c-2cbd6460adf9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@with_time_review\n",
    "def test_RAY(pandas_df, training_yearmonth):\n",
    "    RAY_eval_func = ray.remote(codeblock_evalutation_pipeline)\n",
    "    tasks_list = [\n",
    "        RAY_eval_func.remote(key, df_group, training_yearmonth)\n",
    "        for key, df_group in pandas_df.groupby(\"KEY\")\n",
    "    ]\n",
    "    tasks_list = ray.get(tasks_list)\n",
    "    df_output = pd.concat(tasks_list)\n",
    "    df_output.to_parquet(f\"/dbfs/{OUTPUT_FOLDER}/RAY_PANDAS.parquet\" , index=False)\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "@with_time_review\n",
    "def test_SPARK(spark_df, training_yearmonth):\n",
    "    spark_df_output = spark_df.groupBy(\"KEY\").applyInPandas(\n",
    "        lambda df_group: codeblock_evalutation_pipeline(\n",
    "            key=df_group[\"KEY\"].iloc[0], df_group=df_group, training_yearmonth=training_yearmonth\n",
    "        ), schema=df_schema_spark.schema\n",
    "    )\n",
    "    spark_df_output.write.mode('overwrite').parquet(f\"dbfs:/{OUTPUT_FOLDER}/SPARK_OUTPUT_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e50a1079-621f-43a2-a59f-e55261ea855f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Run the evaluation 5 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59c384ba-318d-49a4-8cb9-e7123a0ce75d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Test: {i}\")\n",
    "    test_RAY(pandas_df=df_dataset, training_yearmonth=training_yearmonth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc5c43a9-75dc-4413-80d1-4eec3925403b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Test: {i}\")\n",
    "    test_SPARK(spark_df=spark_df_dataset, training_yearmonth=training_yearmonth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a51c1acb-3380-4c53-865e-0b7cb218dc6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ray_output = pd.read_parquet(f\"/dbfs/{OUTPUT_FOLDER}/RAY_PANDAS.parquet\")\n",
    "spark_output = pd.read_parquet(f\"/dbfs/{OUTPUT_FOLDER}/SPARK_OUTPUT_parquet\")\n",
    "\n",
    "print( ray_output['RandomForestRegressor'].sum() == spark_output['RandomForestRegressor'].sum())\n",
    "print( ray_output['LGBMRegressor'].sum() == spark_output['LGBMRegressor'].sum() )\n",
    "print( ray_output['XGBRegressor'].sum() == spark_output['XGBRegressor'].sum() )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Example_01_Stallion_Forecasting",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
