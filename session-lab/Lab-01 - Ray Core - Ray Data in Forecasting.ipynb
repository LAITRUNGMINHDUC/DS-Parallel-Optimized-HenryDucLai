{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53950f4b-802b-4ee3-9bd6-5314b6f8f5e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# A usual Forecasting preparation & modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Libraries & Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b17deaf2-067e-4e02-bf43-5377c0c20285",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install openpyxl lightgbm xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83656498-235a-45ee-a3a8-30f86b31d9d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad710e4e-c119-425a-8490-ed826646b6ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Decorator to calculate running time.\n",
    "def with_time_review(func):\n",
    "    import time\n",
    "    import traceback\n",
    "\n",
    "    def wrapper(*args, **kwargs):\n",
    "      begin = time.time()\n",
    "      result = func(*args, **kwargs)\n",
    "      end = round(time.time() - begin, 5)\n",
    "\n",
    "\n",
    "      print (f\"Function: '{func.__name__}' runs for: {end} seconds.\")\n",
    "      print (\"----------------------------\\n\")\n",
    "      return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4851da46-0585-4d32-83ff-859503aad274",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Import parallel libraries\n",
    "import pyspark.pandas as ps_pd\n",
    "import ray\n",
    "\n",
    "# Import machine learning model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgbm\n",
    "import xgboost as xgb\n",
    "\n",
    "# Get Number of CPUs from Spark\n",
    "SPARK_CPUS = sc.defaultParallelism\n",
    "print(SPARK_CPUS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df1fd334-66b9-4626-ac85-fddd82b0c241",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Setup folder and Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%fs ls dbfs:/mnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c22e2009-e11c-4e13-aff4-51eb201393d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_FOLDER = .................\n",
    "os.makedirs('/dbfs/' + OUTPUT_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a4b3789-579c-4731-bdf4-9af94e43b985",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "FILE_PATH = '../dataset/Stallion-AbinBev-kaggle.csv'\n",
    "df_dataset = pd.read_csv(FILE_PATH)\n",
    "df_dataset['KEY'] = df_dataset['Agency'] + '_' + df_dataset['SKU']\n",
    "df_dataset = df_dataset.drop(columns=['Price']) # This PRICE needs to remove due to leakage variables. PRICE = SALES + PROMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1a55e2c-f668-4b4e-9558-b9644cd08806",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Forecast module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afdc3049-bfea-4f1d-a8a8-a51ce13c04e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def codeblock_feature_engineering(key, df_group):\n",
    "    df_group = df_group.sort_values(by=['YearMonth'])\n",
    "    df_group = df_group.reset_index(drop=True)\n",
    "    \n",
    "    for col in ['Sales', 'Promotions']:\n",
    "        for lag in range(1, 13):\n",
    "            df_group[f'f__LAG_{col}_{lag}'] = df_group[col].shift(lag)\n",
    "        for window in [3, 6, 9, 12]:\n",
    "            df_group[f'f__MA_{col}_{window}'] = df_group[col].rolling(window).mean().shift(1)\n",
    "            df_group[f'f__MSTD_{col}_{window}'] = df_group[col].rolling(window).std().shift(1)\n",
    "\n",
    "    df_group = df_group.drop(columns=['Agency', 'SKU'])\n",
    "    df_group = df_group.dropna()\n",
    "    return df_group\n",
    "\n",
    "def codeblock_model_forecasting(key, df_train, df_test):\n",
    "    import numpy as np\n",
    "    np.random.seed(1234)\n",
    "\n",
    "    models_list = [\n",
    "        RandomForestRegressor(n_jobs=1, random_state=1234),\n",
    "        lgbm.LGBMRegressor(n_jobs=1, random_state=1234),\n",
    "        xgb.XGBRegressor(n_jobs=1, random_state=1234),\n",
    "    ]\n",
    "\n",
    "    X_train, y_train = df_train.drop(columns=['Sales']), df_train['Sales']\n",
    "    X_test, y_test = df_test.drop(columns=['Sales']), df_test['Sales']\n",
    "\n",
    "    list_numeric_cols = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "    for model in models_list:\n",
    "        model.fit(X_train[list_numeric_cols], y_train)\n",
    "        y_pred = model.predict(X_test[list_numeric_cols])\n",
    "        df_test[model.__class__.__name__] = y_pred\n",
    "    \n",
    "    return df_test  \n",
    "\n",
    "def codeblock_evalutation_pipeline(key, df_group, training_yearmonth):    \n",
    "\n",
    "    df_feature_engineering = codeblock_feature_engineering(key, df_group)\n",
    "\n",
    "    df_train = df_feature_engineering.query(f\"YearMonth < {training_yearmonth}\")\n",
    "    df_test = df_feature_engineering.query(f\"YearMonth >= {training_yearmonth}\")\n",
    "\n",
    "    df_forecast = codeblock_model_forecasting(key, df_train, df_test)\n",
    "    \n",
    "    return df_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "765b0947-632d-4533-b212-2396f59718db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Demo on 1 Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69db5c66-6337-403e-afba-f4635c5514b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "training_yearmonth = 201707\n",
    "key = 'Agency_01_SKU_01'\n",
    "df_group = df_dataset.query(f\"KEY == '{key}'\")\n",
    "df_demo = codeblock_evalutation_pipeline(key, df_group, training_yearmonth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_spark = spark.createDataFrame(df_demo).schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Man vs. Ray Captain Competition üòè"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark applyInPandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(df_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@with_time_review\n",
    "def test_SPARK(spark_df, training_yearmonth):\n",
    "    spark_df_output = spark_df.groupBy(\"KEY\").applyInPandas(\n",
    "        lambda df_group: codeblock_evalutation_pipeline(\n",
    "            key=df_group[\"KEY\"].iloc[0], df_group=df_group, training_yearmonth=training_yearmonth\n",
    "        ), schema=schema_spark\n",
    "    )\n",
    "    spark_df_output.write.mode('overwrite').parquet(f\"dbfs:/{OUTPUT_FOLDER}/SPARK_OUTPUT_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Test: {i}\")\n",
    "    test_SPARK(spark_df=spark_df, training_yearmonth=training_yearmonth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark applyInPandas with Repartition (The Magic before Ray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_repartition = spark_df.repartition(SPARK_CPUS * 3, 'KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Test: {i}\")\n",
    "    test_SPARK(spark_df=spark_df_repartition, training_yearmonth=training_yearmonth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a06fbe8-7078-4b6c-a21c-77a02da4578c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Parallel setup for Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(..................................)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def ray_verify(i):\n",
    "    return i\n",
    "tasks = [ray_verify.remote(i) for i in range(SPARK_CPUS)]\n",
    "tasks = ray.get(tasks)\n",
    "print(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Number of CPUs from Ray\n",
    "RAY_CPUS = ray.available_resources()\n",
    "\n",
    "print(SPARK_CPUS, ' | ', RAY_CPUS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Core Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@with_time_review\n",
    "def test_RAY_CORE(pandas_df, training_yearmonth):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Test: {i}\")\n",
    "    test_RAY_CORE(pandas_df=df_dataset, training_yearmonth=training_yearmonth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Data Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_df = ray.data.from_pandas(df_dataset)\n",
    "ray_df = ray_df.repartition(RAY_CPUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@with_time_review\n",
    "def test_RAY_DATA(ray_df, training_yearmonth):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Test: {i}\")\n",
    "    test_RAY_DATA(ray_df=ray_df, training_yearmonth=training_yearmonth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e50a1079-621f-43a2-a59f-e55261ea855f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Run the evaluation 5 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a51c1acb-3380-4c53-865e-0b7cb218dc6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ray_output = pd.read_parquet(f\"/dbfs/{OUTPUT_FOLDER}/RAY_PANDAS.parquet\")\n",
    "# spark_output = pd.read_parquet(f\"/dbfs/{OUTPUT_FOLDER}/SPARK_OUTPUT_parquet\")\n",
    "\n",
    "# print( ray_output['RandomForestRegressor'].sum() == spark_output['RandomForestRegressor'].sum())\n",
    "# print( ray_output['LGBMRegressor'].sum() == spark_output['LGBMRegressor'].sum() )\n",
    "# print( ray_output['XGBRegressor'].sum() == spark_output['XGBRegressor'].sum() )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Example_01_Stallion_Forecasting",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
