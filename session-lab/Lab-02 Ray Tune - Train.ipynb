{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee408fad-d227-4b13-ab86-f46ad613b542",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n",
    "\n",
    "os.chdir(\"../../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a310afde-248d-4b4d-952b-0decb9ac2e73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "# from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "# from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import ray\n",
    "from ray.train import lightning as rtl\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray import train, tune\n",
    "import mlflow\n",
    "from ray.air.integrations.mlflow import MLflowLoggerCallback, setup_mlflow\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "import tempfile\n",
    "import torch.nn.functional as F\n",
    "from filelock import FileLock\n",
    "from torchmetrics import Accuracy\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5cdab8c-2ca4-47f9-a8d4-845a4aedb83e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Setup ray cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d80b5cb2-fcf5-4f74-9bd5-149717eb6aed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from ray.util.spark import setup_ray_cluster, shutdown_ray_cluster, MAX_NUM_WORKER_NODES\n",
    "num_cpu_cores_per_worker = 4 # total cpu''s present in each node\n",
    "num_gpu_per_worker = 1 # total gpu''s present in each node\n",
    "use_gpu = True\n",
    "# username = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply()\n",
    "\n",
    "\n",
    "try: \n",
    "  shutdown_ray_cluster()\n",
    "except:\n",
    "  print(\"No Ray cluster is initiated\")\n",
    "\n",
    "try: \n",
    "  ray.shutdown()\n",
    "except:\n",
    "  print(\"No Ray cluster is initiated\")\n",
    "\n",
    "# Start the ray cluster and follow the output link to open the Ray Dashboard - a vital observability tool for understanding your infrastructure and application.\n",
    "setup_ray_cluster(\n",
    "  max_worker_nodes=2, # define your number of worker here\n",
    "  num_cpus_per_node=num_cpu_cores_per_worker,\n",
    "  num_gpus_per_node=num_gpu_per_worker,\n",
    ")\n",
    "runtime_env = {\"pip\": [\"lightning\", \"torch\",\"pytorch_forecasting\"]}\n",
    "ray.init(address=\"auto\", ignore_reinit_error=True,runtime_env=runtime_env)\n",
    "\n",
    "cluster_resources = ray.cluster_resources()\n",
    "print(cluster_resources)\n",
    "\n",
    "num_workers = int(cluster_resources[\"CPU\"] / num_cpu_cores_per_worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d407873-0dc6-4e62-8e27-cbb20075a79b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Model and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dceca8cf-9bf9-4846-8dd0-57f7313e211b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Write custome model and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e0b1924-4526-4f8c-a33e-4fb33874b473",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Using pytorch lightning module to define model \n",
    "class MNISTClassifier(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=10, top_k=1)\n",
    "        self.layer_1_size = config[\"layer_1\"]\n",
    "        self.layer_2_size = config[\"layer_2\"]\n",
    "        self.lr = config[\"lr\"]\n",
    "\n",
    "        # mnist images are (1, 28, 28) (channels, width, height)\n",
    "        self.layer_1 = torch.nn.Linear(28 * 28, self.layer_1_size)\n",
    "        self.layer_2 = torch.nn.Linear(self.layer_1_size, self.layer_2_size)\n",
    "        self.layer_3 = torch.nn.Linear(self.layer_2_size, 10)\n",
    "        self.eval_loss = []\n",
    "        self.eval_accuracy = []\n",
    "\n",
    "    def cross_entropy_loss(self, logits, labels):\n",
    "        return F.nll_loss(logits, labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, width, height = x.size()\n",
    "        x = x.view(batch_size, -1)\n",
    "\n",
    "        x = self.layer_1(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.layer_2(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.layer_3(x)\n",
    "        x = torch.log_softmax(x, dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        accuracy = self.accuracy(logits, y)\n",
    "\n",
    "        self.log(\"ptl/train_loss\", loss)\n",
    "        self.log(\"ptl/train_accuracy\", accuracy)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        accuracy = self.accuracy(logits, y)\n",
    "        self.eval_loss.append(loss)\n",
    "        self.eval_accuracy.append(accuracy)\n",
    "        return {\"val_loss\": loss, \"val_accuracy\": accuracy}\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_loss = torch.stack(self.eval_loss).mean()\n",
    "        avg_acc = torch.stack(self.eval_accuracy).mean()\n",
    "        self.log(\"ptl/val_loss\", avg_loss, sync_dist=True)\n",
    "        self.log(\"ptl/val_accuracy\", avg_acc, sync_dist=True)\n",
    "        self.eval_loss.clear()\n",
    "        self.eval_accuracy.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir=None, batch_size=128):\n",
    "        super().__init__()\n",
    "        if data_dir is None:\n",
    "          self.data_dir = tempfile.mkdtemp()\n",
    "        else: \n",
    "          self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "        )\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        with FileLock(f\"{self.data_dir}.lock\"):\n",
    "            mnist = MNIST(\n",
    "                self.data_dir, train=True, download=True, transform=self.transform\n",
    "            )\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist, [55000, 5000])\n",
    "\n",
    "            self.mnist_test = MNIST(\n",
    "                self.data_dir, train=False, download=True, transform=self.transform\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4f001ae-6020-4795-8ddd-d47695c38dac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Define training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19b37cc4-1fbd-4203-96d5-c320090349bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def tune_train_func(config, data_dir=None, num_gpus=1):\n",
    "    # setup mlflow for logging\n",
    "    \n",
    "    # define model\n",
    "    model = MNISTClassifier(config)\n",
    "    # define the data module/data loader\n",
    "    dm = MNISTDataModule(\n",
    "        data_dir=data_dir, batch_size=config[\"batch_size\"]\n",
    "    )\n",
    "    ########## YOUR CODE HERE ##########\n",
    "    #set up the metrics mapping\n",
    "    \n",
    "    #enable auto logging\n",
    "\n",
    "    # setup the trainer for pytorch lightning\n",
    "    \n",
    "    ########################################\n",
    "    # fit the trainer\n",
    "    trainer.fit(model, dm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34143d85-24aa-4875-9c6e-163adf456b58",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Distributed training with Ray Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efc04e73-fa7e-4088-b9ed-19bab0497889",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def tune_mnist(\n",
    "    config,\n",
    "    num_samples=10,\n",
    "    gpus_per_trial=0,\n",
    "):\n",
    "    data_dir = os.path.join(tempfile.gettempdir(), \"mnist_data_\")\n",
    "    # Download data\n",
    "    MNISTDataModule(data_dir=data_dir, batch_size=config['batch_size']).prepare_data()\n",
    "\n",
    "\n",
    "    ########## YOUR CODE HERE ##########\n",
    "    # Set the MLflow experiment, or create it if it does not exist.\n",
    "\n",
    "\n",
    "    # make your train function work with ray tune\n",
    "    trainable\n",
    "    # define tune config \n",
    "    tune_config\n",
    "    # define run config\n",
    "    run_config \n",
    "\n",
    "    # setup tuner\n",
    "    tuner = \n",
    "    ########################################\n",
    "    # fit the tuner\n",
    "    results = tuner.fit()\n",
    "\n",
    "    print(\"Best hyperparameters found were: \", results.get_best_result().config)\n",
    "    return results.get_best_result().config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65917a3b-c811-459c-a986-9188b7994d5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tune_config = {\n",
    "  \"layer_1\": tune.choice([10, 20, 30]), # add param space\n",
    "  \"layer_2\": tune.choice([20, 30, 40]),\n",
    "  \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "  \"batch_size\": tune.choice([64, 128]),\n",
    "  'num_epochs': 5,\n",
    "  \"tracking_uri\": mlflow.get_tracking_uri(),\n",
    "  \"experiment_id\":\n",
    "  \"experiment_name\":\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf26dba5-a234-4133-8e70-7c962c766a99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "best_config = tune_mnist(config=tune_config,\n",
    "          num_samples=10,\n",
    "          gpus_per_trial=1,\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cadb7ff-d639-49d0-9451-677e9ed99652",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Distributed training with Ray Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dd54097-8779-4bf0-b4ed-596030661382",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Define train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32782e61-00be-4ba2-af66-dbf333bae945",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_func(config):\n",
    "\n",
    "    # define model\n",
    "    model = MNISTClassifier(config)\n",
    "    # define the data module/data loader\n",
    "    dm = MNISTDataModule(data_dir=os.path.join(tempfile.gettempdir(), \"mnist_data_\"), batch_size=config[\"batch_size\"])\n",
    "\n",
    "    ########## YOUR CODE HERE ##########\n",
    "    # setup the trainer for pytorch lightning\n",
    "    \n",
    "    # prepare trainer for ray train\n",
    "\n",
    "    ########################################\n",
    "    # fit the trainer\n",
    "    trainer.fit(model,datamodule=dm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6db7bc7-58c8-4c48-b1df-4ededabfa226",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_config = best_config.copy()\n",
    "train_config['num_epochs'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e531e68-c46d-417e-9cf0-679f034564bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Fit the model with Ray Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66930cde-2300-4c63-9c22-715c70053f03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "########## YOUR CODE HERE ##########\n",
    "# define scaling config\n",
    "scaling_config\n",
    "\n",
    "# define run config \n",
    "run_config\n",
    "########################################\n",
    "trainer = ray.train.torch.TorchTrainer(train_func, train_loop_config=train_config, scaling_config=scaling_config,run_config=run_config)\n",
    "\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af87b92b-9567-417b-a97d-7378ba0d9b1a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Training with single GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50968e7b-771b-4244-b414-07bffdbaa919",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scaling_config = ray.train.ScalingConfig(num_workers=1,\n",
    "                                        use_gpu=True\n",
    "                                        )\n",
    "trainer = ray.train.torch.TorchTrainer(train_func, train_loop_config=train_config, scaling_config=scaling_config,run_config=run_config)\n",
    "\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01875dc1-e4b1-404b-bb20-e2e6b004b8cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3073448887208850,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "[Lab] Ray Tune",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
